# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iz9qdJqgfcqZ-VfSShokKtj06dcnyTaj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Loading the dataset 'credit_card_data.csv'
df = pd.read_csv('creditcard.csv')

#Data Exploration:
#View the first few rows of the dataset
print(df.head())

# Check summary statistics
print(df.describe())

# Count the number of fraudulent and genuine transactions
print(df['Class'].value_counts())

# Visualize the class balance
sns.countplot(x='Class', data=df)
plt.title('Class Distribution')
plt.show()

# Plot histograms for transaction amount and time
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['Amount'], bins=20, kde=True)
plt.title('Transaction Amount Distribution')
plt.xlim(0, 1000)

plt.subplot(1, 2, 2)
sns.histplot(df['Time'], bins=20, kde=True  )
plt.title('Transaction Time Distribution')

plt.tight_layout()
plt.show()

# Summary statistics for each feature
print("Mean:")
print(df.mean())
print("\nStandard Deviation:")
print(df.std())

# Visualize distribution of features by class
plt.figure(figsize=(15, 10))
for i, col in enumerate(df.drop(columns=['Class']), 1):
    plt.subplot(6, 5, i)
    sns.histplot(data=df, x=col, hue='Class', bins=20, kde=True, alpha=0.5, palette='Set1')
    plt.title(col)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Scale the numerical features ('V1' to 'V28', 'Amount', and 'Time')
numerical_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
                      'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
                      'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Time']

df_scaled = df.copy()
df_scaled[numerical_features] = scaler.fit_transform(df_scaled[numerical_features])

# Ensure target variable ('Class') remains unchanged
df_scaled['Class'] = df['Class']

print(df_scaled.head())

from sklearn.model_selection import train_test_split

# Split the data into features (X) and target variable (y)
X = df_scaled.drop(columns=['Class'])
y = df_scaled['Class']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the training and testing sets
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv("creditcard.csv")

# Time Feature Engineering
data['Hour'] = np.floor(data['Time'] / 3600)
data['Hour'] = data['Hour'] % 24
data['Hour_Category'] = pd.cut(data['Hour'], bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'])
data.drop('Time', axis=1, inplace=True)

# Feature Scaling
scaler = StandardScaler()
data['Amount_Scaled'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data.drop('Amount', axis=1, inplace=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv("creditcard.csv")

# Check if the 'Amount' column exists before scaling
if 'Amount' in data.columns:
    # Initialize the StandardScaler
    scaler = StandardScaler()

    # Scale the 'Amount' feature
    data['Amount_Scaled'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))

    # Drop the original 'Amount' feature
    data.drop('Amount', axis=1, inplace=True)
else:
    print("Column 'Amount' does not exist in the DataFrame.")

# Display the modified DataFrame
print(data.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Load the dataset
data = pd.read_csv("creditcard.csv")

# Step 2: Prepare Data
X = data.drop('Class', axis=1)
y = data['Class']

# Step 3: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Data Preprocessing
# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Step 5: Train Model
# Check for missing values in y_train
if y_train.isnull().values.any():
    # Drop samples with missing values from X_train and y_train
    X_train_scaled = X_train_scaled[~y_train.isnull()]
    y_train = y_train.dropna()

# Train Model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

# Step 6: Evaluate Model
y_pred = lr_model.predict(X_test_scaled)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

# Print evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC Score:", roc_auc)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

#Support Vector Machine (SVM)

from sklearn.svm import SVC

# Train Model
svm_model = SVC(kernel='rbf', gamma='scale', random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Evaluate Model
y_pred_svm = svm_model.predict(X_test_scaled)

# Calculate evaluation metrics
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_auc_svm = roc_auc_score(y_test, y_pred_svm)

# Print evaluation metrics
print("SVM Model Evaluation:")
print("Accuracy:", accuracy_svm)
print("Precision:", precision_svm)
print("Recall:", recall_svm)
print("F1 Score:", f1_svm)
print("ROC AUC Score:", roc_auc_svm)

# Confusion Matrix
conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)
print("Confusion Matrix:")
print(conf_matrix_svm)

#model comparision

from sklearn.ensemble import RandomForestClassifier

# Train Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Evaluate Model
y_pred_rf = rf_model.predict(X_test_scaled)

# Calculate evaluation metrics for Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_pred_rf)

# Create a DataFrame to store the evaluation metrics of all models
model_comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Support Vector Machine', 'Random Forest'],
    'Accuracy': [accuracy, accuracy_svm, accuracy_rf],
    'Precision': [precision, precision_svm, precision_rf],
    'Recall': [recall, recall_svm, recall_rf],
    'F1 Score': [f1, f1_svm, f1_rf],
    'ROC AUC Score': [roc_auc, roc_auc_svm, roc_auc_rf]
})

# Print the model comparison table
print("Model Comparison:")
print(model_comparison)

from sklearn.model_selection import GridSearchCV

#Perform hyperparameter tuning for the Logistic Regression model using grid search

# Define hyperparameters grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'penalty': ['l2']
}

# Perform grid search
grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=1000), param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model for evaluation
best_lr_model = grid_search.best_estimator_
y_pred_best = best_lr_model.predict(X_test_scaled)

# Evaluate the best model
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)
roc_auc_best = roc_auc_score(y_test, y_pred_best)

# Print evaluation metrics
print("Evaluation Metrics for Best Model:")
print("Accuracy:", accuracy_best)
print("Precision:", precision_best)
print("Recall:", recall_best)
print("F1 Score:", f1_best)
print("ROC AUC Score:", roc_auc_best)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import joblib

# Step 1: Load the dataset
data = pd.read_csv("creditcard.csv")

# Step 2: Prepare Data
X = data.drop('Class', axis=1)
y = data['Class']

# Step 3: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Data Preprocessing
# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Step 5: Train Model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

# Model Persistence
# Save the trained model using joblib
joblib.dump(lr_model, 'fraud_detection_model.joblib')

# Load the saved model
loaded_model = joblib.load('fraud_detection_model.joblib')

# Predict using the loaded model
# Load new data for prediction from CSV
new_data = pd.read_csv("creditcard.csv")

# Drop any unnecessary columns from the new data (if needed)
new_data = new_data.drop('Class', axis=1)

# Fit the imputer on training data
imputer.fit(X_train)

# Handle missing values in the new data
new_data_imputed = imputer.transform(new_data)

# Scale the features of the new data
new_data_scaled = scaler.transform(new_data_imputed)

# Perform prediction using the loaded model
predictions = loaded_model.predict(new_data_scaled)

# Display the predictions
print("Predictions for new data:")
print(predictions)

# Fit the imputer on the training data
imputer.fit(X_train)

# Handle missing values in the new data
new_data_imputed = imputer.transform(new_data)

# Scale the features of the new data
new_data_scaled = scaler.transform(new_data_imputed)

# Perform prediction using the loaded model
predictions = loaded_model.predict(new_data_scaled)

# Display the predictions
print("Predictions for new data:")
print(predictions)

